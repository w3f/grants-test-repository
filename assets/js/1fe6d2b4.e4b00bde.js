"use strict";(self.webpackChunkgrants=self.webpackChunkgrants||[]).push([[2517],{3905:(e,t,a)=>{a.d(t,{Zo:()=>h,kt:()=>u});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},h=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,h=l(e,["components","mdxType","originalType","parentName"]),m=p(a),u=i,c=m["".concat(s,".").concat(u)]||m[u]||d[u]||o;return a?n.createElement(c,r(r({ref:t},h),{},{components:a})):n.createElement(c,r({ref:t},h))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<o;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},97498:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=a(83117),i=(a(67294),a(3905));const o={},r="Anagolay Project Idiyanale - Phase 1",l={unversionedId:"Applications/anagolay-project-idiyanale-phase-1",id:"Applications/anagolay-project-idiyanale-phase-1",title:"Anagolay Project Idiyanale - Phase 1",description:"- Team Name: Anagolay",source:"@site/docs/Applications/anagolay-project-idiyanale-phase-1.md",sourceDirName:"Applications",slug:"/Applications/anagolay-project-idiyanale-phase-1",permalink:"/grants-test-repository/Applications/anagolay-project-idiyanale-phase-1",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"defaultSidebar",previous:{title:"Ajuna Network Follow up",permalink:"/grants-test-repository/Applications/ajuna_network_follow_up"},next:{title:"Ares",permalink:"/grants-test-repository/Applications/ares_protocol"}},s={},p=[{value:"Overview",id:"overview",level:2},{value:"Operation",id:"operation",level:3},{value:"Operation Version",id:"operation-version",level:4},{value:"Workflow",id:"workflow",level:3},{value:"On decentralized storage",id:"on-decentralized-storage",level:3},{value:"Example of how this approach affects other things",id:"example-of-how-this-approach-affects-other-things",level:3},{value:"Workflow for image PoE",id:"workflow-for-image-poe",level:3},{value:"Project Details",id:"project-details",level:2},{value:"Ecosystem Fit",id:"ecosystem-fit",level:2},{value:"Team :busts_in_silhouette",id:"team-busts_in_silhouette",level:2},{value:"Team members",id:"team-members",level:3},{value:"Contact",id:"contact",level:3},{value:"Legal Structure",id:"legal-structure",level:3},{value:"Team&#39;s experience",id:"teams-experience",level:3},{value:"Daniel Maricic",id:"daniel-maricic",level:4},{value:"Adriano Dalpane",id:"adriano-dalpane",level:4},{value:"Team Code Repos",id:"team-code-repos",level:3},{value:"Team LinkedIn Profiles (if available)",id:"team-linkedin-profiles-if-available",level:3},{value:"Overview",id:"overview-1",level:2},{value:"Milestone 1 \u2014 Implement core functionality",id:"milestone-1--implement-core-functionality",level:2},{value:"Substrate module - an_operation",id:"substrate-module---an_operation",level:3},{value:"Operation - op_file",id:"operation---op_file",level:3},{value:"Anagolay CLI: Operation Part 1",id:"anagolay-cli-operation-part-1",level:3},{value:"Milestone 2 \u2014 Implementing the Workflow pallet, execution, manifest generation, and CID and Multihash Operations",id:"milestone-2--implementing-the-workflow-pallet-execution-manifest-generation-and-cid-and-multihash-operations",level:2},{value:"Substrate module - an_workflow",id:"substrate-module---an_workflow",level:3},{value:"Anagolay CLI: Workflow manifest generation",id:"anagolay-cli-workflow-manifest-generation",level:3},{value:"Operation - op_cid",id:"operation---op_cid",level:3},{value:"Operation - op_multihash",id:"operation---op_multihash",level:3},{value:"Workflow: execution",id:"workflow-execution",level:3}],h={toc:p};function d(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"anagolay-project-idiyanale---phase-1"},"Anagolay Project Idiyanale - Phase 1"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Team Name:")," Anagolay"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Payment Address:")," ",(0,i.kt)("inlineCode",{parentName:"li"},"0x3da76bca7ccc8f92dc9c3bc000ea5fc64d7f76b8")," (USDT)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("a",{parentName:"strong",href:"https://github.com/w3f/Grants-Program/tree/master#level_slider-levels"},"Level"),":")," 2")),(0,i.kt)("h1",{id:"project-overview-page_facing_up"},"Project Overview :page_facing_up"),(0,i.kt)("p",null,"We applied for a grant in 2020 and got approved under the name ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/w3f/Grants-Program/pull/5"},"Sensio Network"),". Since then we did a lot of research and improved the approach. We did rebranding; SensioPhoto is ",(0,i.kt)("a",{parentName:"p",href:"http://Kelp.Digital"},"Kelp Digital")," and SensioNetwork is ",(0,i.kt)("a",{parentName:"p",href:"http://Anagolay.Network"},"Anagolay Network"),"."),(0,i.kt)("h2",{id:"overview"},"Overview"),(0,i.kt)("p",null,"Anagolay is a peer-to-peer network that stores records of Rights (Copyright, Licenses, and Ownership), Restrictions, and Proofs of any digital content. It empowers the users to store, claim, sell, and rent their work with the correct transfer of Rights and usage of Licenses. Actual digital data is ",(0,i.kt)("em",{parentName:"p"},"never")," stored on the chain, only the respective cryptographic proof. As such, it acts as an identifier, verifiable by users who have access to that same data without disclosing it in the process."),(0,i.kt)("p",null,"In this grant we will focus on the core building blocks, ",(0,i.kt)("strong",{parentName:"p"},"Operation")," and a ",(0,i.kt)("strong",{parentName:"p"},"Workflow"),"."),(0,i.kt)("h3",{id:"operation"},"Operation"),(0,i.kt)("p",null,"Operation is a well-structured library with standardized input and output signatures, written in Rust and compiled to WASM. It acts as a lego piece, which you use to connect to other pieces, creating a Workflow. Operation itself consists of two parts, a Manifest and a ",(0,i.kt)("a",{parentName:"p",href:"#operation-version"},"Version"),". The Version uses a content versioning system which allows it to be used as a dependency exactly as it is, always. This solves the problem when a dependency introduces breaking changes at the patch version or gets sold to a bad actor and they ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/greatsuspender/thegreatsuspender/issues/1263"},"ship malicious code"),", or it's ",(0,i.kt)("a",{parentName:"p",href:"https://www.theregister.com/2016/03/23/npm_left_pad_chaos/"},"removed")," from the centralized registry. The Manifest contains the data which describe the Operation inputs, outputs, potential configuration, execution group, and a few other fields for developers, like description and name. Once Operation is created, the Manifest cannot be updated and all Versions must adhere to the Manifest entirely. There are three types of Operations: ",(0,i.kt)("inlineCode",{parentName:"p"},"User"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"FlowControl"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"System"),". In this grant, we will focus on ",(0,i.kt)("inlineCode",{parentName:"p"},"System")," Operations and their execution."),(0,i.kt)("h4",{id:"operation-version"},"Operation Version"),(0,i.kt)("p",null,"It is the source code that conforms and implements the interfaces and functionality explained in the Manifest. This is what gets published and compiled and what is used in the Workflow execution. Since all the Operation Versions have the same Manifest, what can differ is the underlying improvements to either performance or dependency update. Each new version goes through the same verification process as if it were the first one, ",(0,i.kt)("em",{parentName:"p"},"this will not be implemented in this grant"),", which at the end marks the version as ",(0,i.kt)("inlineCode",{parentName:"p"},"approved"),". This approach minimizes the introduction of bad code and hold everybody involved responsible for code quality."),(0,i.kt)("h3",{id:"workflow"},"Workflow"),(0,i.kt)("p",null,"Workflow is a collection of Operations, chained together producing a tree-like structure where the leaf Operation is the entry-point. Once executed, they produce the list of strings that act as the input data identifiers, we call them ",(0,i.kt)("strong",{parentName:"p"},"Proofs"),". The way how Operations are linked together is by following this rule ",(0,i.kt)("inlineCode",{parentName:"p"},"OperationA.outputType === OperationB.inputType"),", this is a standardized interface that every Operation must implement. Execution is performed from bottom to top, where the most bottom operation acts as an entry point for receiving the data. When Operation is executed the Workflow execution function will pass the output as an input to the parent Operation, and so on until the execution reaches the top level when it stops. The Workflow doesn't contain the Operation manifest, rather direct links to the Versions.\nYou can see ",(0,i.kt)("a",{parentName:"p",href:"#workflow-for-image-poe"},"here")," a diagram how one Workflow might look like."),(0,i.kt)("p",null,"Execution"),(0,i.kt)("p",null,"The most important environment for Workflow execution is Browser (WebWorkers) and Nodejs. All the Operations are compiled to WASM which can be run almost ",(0,i.kt)("a",{parentName:"p",href:"https://webassembly.org/roadmap/"},"anywhere"),". Each Operation passes the data to its parent as a ",(0,i.kt)("inlineCode",{parentName:"p"},"bincode ByteArray"),", then the parent Operation deserializes it and uses it. It's the most efficient way we found which still keeps the execution in optimal speed limits."),(0,i.kt)("h3",{id:"on-decentralized-storage"},"On decentralized storage"),(0,i.kt)("p",null,"Decentralized storage is a complex topic where many projects contribute in different ways. In the whole ecosystem, there are two distinct subdomains, ones with the token, and one without. Essentially all of them are trying to find the best way to incentivize people to store other people's data, to make sure the data stays available over a long period of time via contract-based approach like FileCoin and Crust, or store data permanently like Arweave. Some of the projects have high entry hardware requirements, some little bit less, but all of them agree that unused harddrive resources can be utilized and contributed to the respective network. Each of the projects gives guarantees about the data durability, either through the on-chain ",(0,i.kt)("em",{parentName:"p"},"Deal")," or as a default, in case of permanent storage."),(0,i.kt)("p",null,"Now when we explained the projects that contain the token/coin, let's talk about the vanilla IPFS and how it compares to others. Based on the image below, you can see that some of the projects mentioned above are built on top of, or work with the IPFS. This gives them out-of-the-box all of the IPFS features. What IPFS doesn't provide, is the guarantee that the uploaded content will be available forever, BUT it can, it all depends on the people who host it without any monetary incentive, rather than the reason they want to host it. Also, there is a lot smaller hardware requirement for hosting a normal IPFS node than being a miner/validator for big projects mentioned above."),(0,i.kt)("p",null,"Thing is that, why not use both? One as a temporary, short-term, high-volume storage of not so important data and another as permanent storage for the data we want to keep forever."),(0,i.kt)("p",null,"This is the approach we are taking in Anagolay, storing the important data like Operation Version and Workflow Manifest. Our main requirements regarding decentralized storage are:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"content addressability or IPFS based"),(0,i.kt)("li",{parentName:"ul"},"permanency"),(0,i.kt)("li",{parentName:"ul"},"high durability ( at least 11 9s )")),(0,i.kt)("p",null,"At the time of writing, there not a single project that provides the all three requirements, although Arweave + IPFS can work. We will use the Anagolay-hosted public IPFS cluster which will be used as short-term storage and as permanent storage until we have the ",(0,i.kt)("em",{parentName:"p"},"real")," permanent storage connected."),(0,i.kt)("p",null,"Even though this poses the problem of centralization, where we host all the Operations and their source code, nothing prevents developers who build their Operations and Workflows, to spin up the IPFS node and pin only the content they are interested in. This also applies to any other dApp, they can personally host IPFS node or use Infura and friends, to pin the Operations and Workflows they depend on. ",(0,i.kt)("a",{parentName:"p",href:"https://kelp.notion.site/FileCoin-vs-Crust-vs-Arweave-vs-IPFS-d5d4943e66e74f1381b36b28bdc030bd"},"Here")," is a page we maintain, which contains the comparison table of decentralized storage providers."),(0,i.kt)("h3",{id:"example-of-how-this-approach-affects-other-things"},"Example of how this approach affects other things"),(0,i.kt)("p",null,"The most obvious examples where this approach is a game-changer are the NFT marketplaces. Especially when it comes to the definition of uniqueness. NFT is considered to be a unique thing, but it's not. What is unique is the identifier and not the content. It is possible to mint the same image as an NFT on different marketplaces ( ",(0,i.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=ntyZ1tCy9Is"},"Kelp Digital Are NFTs as Unique as We Think? video")," ) and on some marketplaces slightly modified image you can mint again, this alone states that there is no uniqueness. What we are building is a way to determine the uniqueness of the digital content. The Proofs are plural, they are the describing identifiers of the image. When this gets implemented, the NFT will be obsolete and either die out or become something else. Why? Because we rely on the identifiers of the content rather than on the incrementing value which is obtained through the minting process and that makes every image or digital data potential NFT without minting. Third parties ( current NFT marketplaces ) can execute specific Workflow to obtain the Proofs, which they can store internally, or query Anagolay Network to see are these Proofs match any records with claimed Copyrights or Ownerships. Due to the nature of the Operation and its return, not every time, all Proofs will match, especially for an image; remember that the 2 out of 3 are cryptographic proofs, and one is ",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Locality-sensitive_hashing"},"LSH")," proof which can be used to calculate the similarity between the original image data and one that is already claimed. We can say that the distance of 0.98 will be considered as valid Proof of equality which means that the uploaded data is already known to the system and already has the owner. Example Workflow is ",(0,i.kt)("a",{parentName:"p",href:"#workflow-for-image-poe"},"here")),(0,i.kt)("h3",{id:"workflow-for-image-poe"},"Workflow for image PoE"),(0,i.kt)("p",null,"Here is a working example of the Workflow, represented as a diagram for the dependencies only which at the end of the execution provide 3 proofs; 2 CIDs and 1 Perceptual hash. These Proofs are tied together and represent the identifiers of an image.  "),(0,i.kt)("p",null,"Image is located ",(0,i.kt)("a",{parentName:"p",href:"https://bafybeieh5nuwrr2f75ejf4nalc5xyhrspj67yl7ha27toupsrg5wnowedm.ipfs.dweb.link/universe.jpg"},"here")),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://bafybeihrkjwjgfqrhnlgtmjbd3zkfmdzyqmbdjsqj3hghxtiobi7ng7ilm.ipfs.dweb.link/AnagolayNetwork-Workflow_for_PoE_of_an_image.jpg",alt:"Diagram of a workflow that creates the PoE of any image"})),(0,i.kt)("hr",null),(0,i.kt)("h2",{id:"project-details"},"Project Details"),(0,i.kt)("p",null,"We expect the teams to already have a solid idea about your project's expected final state. Therefore, we ask the teams to submit (where relevant):"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Mockups/designs of any UI components",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"N/A"))),(0,i.kt)("li",{parentName:"ul"},"Data models / API specifications of the core functionality",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"N/A"))),(0,i.kt)("li",{parentName:"ul"},"An overview of the technology stack to be used",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Substrate, Rust, Wasm, wasm-bindgen, Typescript, Svelte or React"))),(0,i.kt)("li",{parentName:"ul"},"Documentation of core components, protocols, architecture, etc. to be deployed",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"After every milestone, we will deploy the deliverables on our infra for testing."))),(0,i.kt)("li",{parentName:"ul"},"PoC/MVP or other relevant prior work or research on the topic",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"check ",(0,i.kt)("a",{parentName:"li",href:"https://anagolay.dev"},"Anagolay Dev")))),(0,i.kt)("li",{parentName:"ul"},"What your project is ",(0,i.kt)("em",{parentName:"li"},"not")," or will ",(0,i.kt)("em",{parentName:"li"},"not")," provide or implement",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"The scope of the project is experimental and is not going to produce a production-ready artifact")))),(0,i.kt)("h2",{id:"ecosystem-fit"},"Ecosystem Fit"),(0,i.kt)("p",null,"Help us locate your project in the Polkadot/Substrate/Kusama landscape and what problems it tries to solve by answering each of these questions:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Where and how does your project fit into the ecosystem?",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Substrate based projects ( standalone pallets )",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"can include and use the Anagolay pallets if they wish to maintain the Proofs and Statements for themselves"),(0,i.kt)("li",{parentName:"ul"},"can include and use the Anagolay pallets to store and build the Workflows and Operations"))),(0,i.kt)("li",{parentName:"ul"},"Substrate based projects ( using Anagolay chain )",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"define their Workflows which are stored on the Anagolay chain and then execute them on the runtime or in off-chain-workers"),(0,i.kt)("li",{parentName:"ul"},"chains that require Rights management for Copyrights and Ownerships of digital content where they would create them and assign to the users"),(0,i.kt)("li",{parentName:"ul"},"using the Anagolay Workflows on any rust or wasm able runtime"))))),(0,i.kt)("li",{parentName:"ul"},"Who is your target audience (parachain/dapp/wallet/UI developers, designers, your own user base, some dapp's userbase, yourself)?",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"any chain, dApp, or a hybrid app that requires the Rights management for Copyrights and Ownerships"),(0,i.kt)("li",{parentName:"ul"},"ourselves with the ",(0,i.kt)("a",{parentName:"li",href:"https://kelp.digital"},"Kelp.digital")," project"))),(0,i.kt)("li",{parentName:"ul"},"What need(s) does your project meet?",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Rights management for Copyrights and Ownerships"),(0,i.kt)("li",{parentName:"ul"},"Verifiable Proofs using the transparent process ( Workflow )  "))),(0,i.kt)("li",{parentName:"ul"},"Are there any other projects similar to yours in the Substrate / Polkadot / Kusama ecosystem?",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"NONE")))),(0,i.kt)("h2",{id:"team-busts_in_silhouette"},"Team :busts_in_silhouette"),(0,i.kt)("h3",{id:"team-members"},"Team members"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Daniel Maricic"),(0,i.kt)("li",{parentName:"ul"},"Adriano Dalpane")),(0,i.kt)("h3",{id:"contact"},"Contact"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Contact Name:")," Daniel Maricic"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Contact Email:")," ",(0,i.kt)("a",{parentName:"li",href:"mailto:daniel@woss.io"},"daniel@woss.io")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Website:")," ",(0,i.kt)("a",{parentName:"li",href:"https://anagolay.network"},"https://anagolay.network"))),(0,i.kt)("h3",{id:"legal-structure"},"Legal Structure"),(0,i.kt)("p",null,"No legal structure yet. We are planning to create an Anagolay Foundation in the future. All IP is held by Daniel Maricic which he will transfer to the Anagolay Foundation at the time of its creation."),(0,i.kt)("h3",{id:"teams-experience"},"Team's experience"),(0,i.kt)("h4",{id:"daniel-maricic"},"Daniel Maricic"),(0,i.kt)("p",null,"An IT generalist with 13 years of on-hand experience in different business domains and building large and scalable applications from Fintech to real-time auctions and image management software. Four years ago he came up with the idea to create software that would help him share his photos with built-in copyrights and licenses. That idea evolved into two distinct projects, Anagolay and Kelp."),(0,i.kt)("h4",{id:"adriano-dalpane"},"Adriano Dalpane"),(0,i.kt)("p",null,"He is a developer with 11 years of experience in the domain of several telecom operators and the travel industry. Passionate about AI, he implemented a Monte Carlo Tree Search artificial intelligence for a game and a Bayesian classifier to perform match-making on a marketplace platform. He has been working with Rust and Substrate blockchain technologies for about two years and more recently he joined the Anagolay team."),(0,i.kt)("h3",{id:"team-code-repos"},"Team Code Repos"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://gitlab.com/anagolay/docs"},"https://gitlab.com/anagolay/docs")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/anagolay/anagolay-chain"},"https://github.com/anagolay/anagolay-chain")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/anagolay/js-sdk"},"https://github.com/anagolay/js-sdk"))),(0,i.kt)("p",null,"Please also provide the GitHub accounts of all team members. If they contain no activity, references to projects hosted elsewhere or live are also fine."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/woss"},"https://github.com/woss")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/digitalillusion"},"https://github.com/digitalillusion"))),(0,i.kt)("h3",{id:"team-linkedin-profiles-if-available"},"Team LinkedIn Profiles (if available)"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.linkedin.com/in/danielmaricic/"},"https://www.linkedin.com/in/danielmaricic/")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.linkedin.com/in/adriano-dalpane/"},"https://www.linkedin.com/in/adriano-dalpane/"))),(0,i.kt)("h1",{id:"development-roadmap-nut_and_bolt"},"Development Roadmap :nut_and_bolt"),(0,i.kt)("h2",{id:"overview-1"},"Overview"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Total Estimated Duration:")," ",(0,i.kt)("del",{parentName:"li"},"2 months")," 5 months in total"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Full-Time Equivalent (FTE):")," 2 FTE"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Total Costs:")," 27,500 USDT")),(0,i.kt)("h2",{id:"milestone-1--implement-core-functionality"},"Milestone 1 \u2014 Implement core functionality"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Estimated duration:")," 1 month"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"FTE:")," 2"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Costs:")," 10,000 USDT")),(0,i.kt)("p",null,"This milestone will set the base for the next milestones.\n| Number | Deliverable                      | Specification                                                                                                                                                                                    |\n| -----: | -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n|    0a. | License                          | All ",(0,i.kt)("inlineCode",{parentName:"p"},"an_")," prefixes will be GPLv3, all ",(0,i.kt)("inlineCode",{parentName:"p"},"op_")," Apache2                                                                                                                                              |\n|    0b. | Documentation                    | We will provide both inline documentation of the code and a basic tutorial that explains how a developer can create Operations and their versions. How to store them on the chain and query them |\n|    0c. | Testing Guide                    | Core functions will be fully covered by unit tests to ensure functionality and robustness. In the guide, we will describe how to run these tests.                                                |\n|    0d. | Docker                           | We will provide a Dockerfile that can be used to test all the functionality delivered with this milestone.                                                                                       |\n|    0e. | Article                          | We will publish an article that explains what was done as part of the grant, with the focus on the developers' community                                                                         |\n|     1. | Substrate module: ",(0,i.kt)("inlineCode",{parentName:"p"},"an_operation")," | ",(0,i.kt)("a",{parentName:"p",href:"#substrate-module---an_operation"},"See here"),"                                                                                                                                                     |\n|     2. | Benchmarks: ",(0,i.kt)("inlineCode",{parentName:"p"},"an_operation"),"       | Improving the benchmarking and re-calculation.                                                                                                                                                   |\n|     3. | Anagolay CLI: Operation Part 1   | ",(0,i.kt)("a",{parentName:"p",href:"#anagolay-cli-operation-part-1"},"See here"),"                                                                                                                                                       |\n|     4. | Operation: ",(0,i.kt)("inlineCode",{parentName:"p"},"op_file"),"             | ",(0,i.kt)("a",{parentName:"p",href:"#operation---op_file"},"See here"),"                                                                                                                                                                 |\n|     5. | Rust demo crate - Part 1         | Part 1 of the rust demo crate. Setup the initial structure for the demo as a lib and binary.                                                                                                     |"),(0,i.kt)("h3",{id:"substrate-module---an_operation"},"Substrate module - an_operation"),(0,i.kt)("p",null,"We will create a Substrate pallet that will contain the storage and extrinsics for creating the ",(0,i.kt)("inlineCode",{parentName:"p"},"Operation"),"s and ",(0,i.kt)("inlineCode",{parentName:"p"},"OperationVersion"),"s and their storage items. The storage items include all the stores we will need (not 100% decided yet) and mapping storage for ",(0,i.kt)("inlineCode",{parentName:"p"},"OperationVersion <-> Operation"),". The list of extrinsics is not 100% defined yet and what we have might be subject to change, but what we have defined is the following:"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Extrinsics"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"create")," - This extrinsic is used to store the Operation, their version, and the mapping. All mandatory checks are run before the saving. The submitting account ID will be the owner of the data."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"approve_version")," - This extrinsic is used to approve the OperationVersion after a successful test from the community. This part will come later and this is used now to build a base for new features and as a rule where every OperationVersion must be approved before being used in the Workflow  ")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Storage"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Operations")," -  ",(0,i.kt)("inlineCode",{parentName:"li"},"StorageDoubleMap")," is a mapping between the ",(0,i.kt)("inlineCode",{parentName:"li"},"Operation_CID"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"AccountID"),", and the actual data"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"OperationCount")," -  ",(0,i.kt)("inlineCode",{parentName:"li"},"StorageValue")," a ",(0,i.kt)("inlineCode",{parentName:"li"},"u64")," incremented when we create Operation"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"OperationVersions")," - ",(0,i.kt)("inlineCode",{parentName:"li"},"StorageMap"),", a mapping between the ",(0,i.kt)("inlineCode",{parentName:"li"},"Operation_CID")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"Vec<OperationVersion>"),", where the last item is considered to be the latest")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://anagolay.dev/about/operation/"},"V1 of Operation is explained here")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"#milestone-1--implement-core-functionality"},"\u25c0\ufe0f Go back to Milestone 1")),(0,i.kt)("h3",{id:"operation---op_file"},"Operation - op_file"),(0,i.kt)("p",null,"We will create an Anagolay operation called file. This operation can take a string or a file buffer. In the case of the string, it will read the file and return the file buffer instance, in the case of a buffer it will return the correct instance. This way we can make sure that all targeted environments are using the same file reading approach and correct return data. Of course, the wasm will accept the ",(0,i.kt)("inlineCode",{parentName:"p"},"ArrayBuffer")," and it will be correctly returned for any other operation that is executed after in the browser environment. Nodejs and Rust can simply pass the file path as a string and the ",(0,i.kt)("inlineCode",{parentName:"p"},"op_file")," will read it."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"#milestone-1--implement-core-functionality"},"\u25c0\ufe0f Go back to Milestone 1")),(0,i.kt)("h3",{id:"anagolay-cli-operation-part-1"},"Anagolay CLI: Operation Part 1"),(0,i.kt)("p",null,"The purpose of the CLI is to build the Operation artifacts, rehost the repository, store all the links to the Anagolay chain."),(0,i.kt)("p",null,"We will implement this list of features:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"build WASM with ",(0,i.kt)("inlineCode",{parentName:"li"},".d.ts")," for",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"web environment as per ",(0,i.kt)("a",{parentName:"li",href:"https://rustwasm.github.io/docs/wasm-bindgen/reference/deployment.html#without-a-bundler"},"wasm-bindgen doc")),(0,i.kt)("li",{parentName:"ul"},"nodejs environment as per ",(0,i.kt)("a",{parentName:"li",href:"https://rustwasm.github.io/docs/wasm-bindgen/reference/deployment.html#nodejs"},"wasm-bindgen doc")))),(0,i.kt)("li",{parentName:"ul"},"Upload the artifacts to the IPFS"),(0,i.kt)("li",{parentName:"ul"},"Store the info on the chain",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Operation Version"),(0,i.kt)("li",{parentName:"ul"},"Operation")))),(0,i.kt)("p",null,"The following code snippet illustrates how the process should look like."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"\n > anagolay operation publish\nPacking the operation op_file ... \nPacking is done \u2705\n\nPublishing ...\nPublishing is done \u2705\n\nSaving to the Anagolay network...\nSaving is done \u2705\n\n\nOperation published and the ID is bafybeifcmrf2ulnwdrhjpkwi2ifbixegegcs22rqbvlzhlcfklzw2ye4fu \n")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"#milestone-1--implement-core-functionality"},"\u25c0\ufe0f Go back to Milestone 1")),(0,i.kt)("h2",{id:"milestone-2--implementing-the-workflow-pallet-execution-manifest-generation-and-cid-and-multihash-operations"},"Milestone 2 \u2014 Implementing the Workflow pallet, execution, manifest generation, and CID and Multihash Operations"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Estimated duration:")," ",(0,i.kt)("del",{parentName:"li"},"1 month")," actual 3.5 months"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"FTE:")," 2"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Costs:")," 17,500 USDT")),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:"right"},"Number"),(0,i.kt)("th",{parentName:"tr",align:null},"Deliverable"),(0,i.kt)("th",{parentName:"tr",align:null},"Specification"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"0a."),(0,i.kt)("td",{parentName:"tr",align:null},"License"),(0,i.kt)("td",{parentName:"tr",align:null},"All ",(0,i.kt)("inlineCode",{parentName:"td"},"an_")," prefixes will be GPLv3, all ",(0,i.kt)("inlineCode",{parentName:"td"},"op_")," Apache2")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"0b."),(0,i.kt)("td",{parentName:"tr",align:null},"Documentation"),(0,i.kt)("td",{parentName:"tr",align:null},"We will provide both inline documentation of the code and a basic tutorial that explains how a developer can create Operations and their versions. How to store them on the chain and query them")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"0c."),(0,i.kt)("td",{parentName:"tr",align:null},"Testing Guide"),(0,i.kt)("td",{parentName:"tr",align:null},"Core functions will be fully covered by unit tests to ensure functionality and robustness. In the guide, we will describe how to run these tests.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"0d."),(0,i.kt)("td",{parentName:"tr",align:null},"Docker"),(0,i.kt)("td",{parentName:"tr",align:null},"We will provide a Dockerfile that can be used to test all the functionality delivered with this milestone.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"0e."),(0,i.kt)("td",{parentName:"tr",align:null},"Article"),(0,i.kt)("td",{parentName:"tr",align:null},"We will publish an article that explains what was done as part of the grant, with the focus on the developers' community")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"1."),(0,i.kt)("td",{parentName:"tr",align:null},"Substrate module: ",(0,i.kt)("inlineCode",{parentName:"td"},"an_workflow")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"#substrate-module---an_workflow"},"See here"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"2."),(0,i.kt)("td",{parentName:"tr",align:null},"Benchmarks: ",(0,i.kt)("inlineCode",{parentName:"td"},"an_workflow")),(0,i.kt)("td",{parentName:"tr",align:null},"Creating the benchmarking")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"3."),(0,i.kt)("td",{parentName:"tr",align:null},"Anagolay CLI: workflow manifest generation"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"#anagolay-cli-workflow-manifest-generation"},"See here"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"5."),(0,i.kt)("td",{parentName:"tr",align:null},"Operation: ",(0,i.kt)("inlineCode",{parentName:"td"},"op_cid")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"#operation---op_cid"},"See here"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"6."),(0,i.kt)("td",{parentName:"tr",align:null},"Operation: ",(0,i.kt)("inlineCode",{parentName:"td"},"op_multihash")),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"#operation---op_multihash"},"See here"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"7."),(0,i.kt)("td",{parentName:"tr",align:null},"Workflow: execution"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("a",{parentName:"td",href:"#workflow-execution"},"See here"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"8."),(0,i.kt)("td",{parentName:"tr",align:null},"Demo nodejs app part 1"),(0,i.kt)("td",{parentName:"tr",align:null},"Creating the nodejs app which will use implemented operations as WASM and produce the CID of an image")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:"right"},"9."),(0,i.kt)("td",{parentName:"tr",align:null},"Rust demo crate - Part 2"),(0,i.kt)("td",{parentName:"tr",align:null},"Creating the rust crate which will use the implemented Operation as a rust library to read a file and generate the CID.")))),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"NOTE"),": All the apps, ",(0,i.kt)("em",{parentName:"p"},"Nodejs"),", and ",(0,i.kt)("em",{parentName:"p"},"Rust demo crate")," when executed must produce the same CID. WHY? Because if they use the same data and the same Workflow they must produce the same output. Same Workflow means using the same Operation manifest which means using the same Operation Version. 100% the same code execution!")),(0,i.kt)("h3",{id:"substrate-module---an_workflow"},"Substrate module - an_workflow"),(0,i.kt)("p",null,"We will be implementing the ",(0,i.kt)("inlineCode",{parentName:"p"},"an_workflow")," pallet which will contain extrinsics and storage. This pallet is used to store and retrieve the Workflow manifest which is then used by developers to create or verify the set of proofs. The ",(0,i.kt)("a",{parentName:"p",href:"#workflow-execution"},"Workflow execution")," depends on this pallet and its storage. For Workflow explanation click ",(0,i.kt)("a",{parentName:"p",href:"#workflow-explanation"},"here"),"."),(0,i.kt)("h3",{id:"anagolay-cli-workflow-manifest-generation"},"Anagolay CLI: Workflow manifest generation"),(0,i.kt)("p",null,"We will build an interactive CLI which will be used to generate the Workflow manifest, validate it and store it on the chain. The CLI will be written in Typescript for Nodejs environment and published on IPFS and maybe NPM. The exact structure is not yet defined but here is the idea of what it should look like:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},"\n > anagolay workflow create\nPlease select the starting operation from the list:\n\u2705: op_file\n\ud83d\udfe2: op_multihash\n\ud83d\udfe2: op_cid\n\nDo you want to add a dependency? (Y:n)\nPlease select the next operation from the list:\n\u2705: op_multihash (blake3)\n\u2705: op_multihash (sha2)\n\u2705: op_multihash (blake2b)\n\nEnter Workflow name: Anagolay Image PoE workflow\nEnter Workflow description: Proof of existence for any image with multiple identifiers\n\nAre you done? (Y:n)\n\nSaving to the Anagolay network...\n\nWorkflow created with the ID: bafybeifcmrf2ulnwdrhjpkwi2ifbixegegcs22rqbvlzhlcfklzw2ye4au \n")),(0,i.kt)("p",null,'As you can see the second question shows only the operations that can be connected to the previously selected operation. Example of the "Image PoE Workflow" is ',(0,i.kt)("a",{parentName:"p",href:"#workflow-for-image-poe"},"here")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"#milestone-2--implementing-the-workflow-pallet-execution-manifest-generation-and-cid-and-multihash-operations"},"\u25c0\ufe0f Go back to Milestone 2")),(0,i.kt)("h3",{id:"operation---op_cid"},"Operation - op_cid"),(0,i.kt)("p",null,"We will create an Anagolay operation called ",(0,i.kt)("inlineCode",{parentName:"p"},"op_cid"),". This takes any Multihash type and generates the CID with the multicodec set as ",(0,i.kt)("inlineCode",{parentName:"p"},"RAW")," and the multibase set as ",(0,i.kt)("inlineCode",{parentName:"p"},"base32"),"."),(0,i.kt)("h3",{id:"operation---op_multihash"},"Operation - op_multihash"),(0,i.kt)("p",null," We will create an Anagolay operation called ",(0,i.kt)("inlineCode",{parentName:"p"},"op_multihash"),". The Operation takes a buffer and creates the multihash instance. Possible multihashes will be ",(0,i.kt)("inlineCode",{parentName:"p"},"sha2-256"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"blake2b-256"),", and ",(0,i.kt)("inlineCode",{parentName:"p"},"blake3")),(0,i.kt)("h3",{id:"workflow-execution"},"Workflow: execution"),(0,i.kt)("p",null,"Execution of the Workflow manifest created in ",(0,i.kt)("a",{parentName:"p",href:"#anagolay-cli-workflow-manifest-generation"},"#3"),". We will implement basic recursive and automatic execution of the Workflow only for ",(0,i.kt)("inlineCode",{parentName:"p"},"SYSTEM")," Operations. The execution will load all the dependencies and execute them in the correct order.  We will NOT implement any kind of optimizations like caching or memoization to gain a boost in speed.  "),(0,i.kt)("h1",{id:"future-plans"},"Future Plans"),(0,i.kt)("p",null,"Later, we will implement the social aspect of the Operation and Workflow credibility creating the revenue streams for developers who will be testing the Operations and the developer who creates the Operation. This is part of the Reputation system for the Developers, Operations, and Workflows."),(0,i.kt)("h1",{id:"additional-information-heavy_plus_sign"},"Additional Information :heavy_plus_sign"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"How did you hear about the Grants Program?")),(0,i.kt)("p",null,"We already applied for a grant which got selected and approved. The details are in the ",(0,i.kt)("a",{parentName:"p",href:"#project-overview-page_facing_up"},"Project Overview")),(0,i.kt)("p",null,"Here you can also add any additional information that you think is relevant to this application but isn't part of it already, such as:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Work you have already done.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"We have done the V1 of the Operations which are currently written in Typescript. They didn't scale and they needed to revamp."),(0,i.kt)("li",{parentName:"ul"},"JS SDK written in Typescript is created to support V1 and used in our pilot project ",(0,i.kt)("a",{parentName:"li",href:"https://kelp.digital"},"Kelp.digital")," which is still in the development phase."))),(0,i.kt)("li",{parentName:"ul"},"If there are any other teams who have already contributed (financially) to the project.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Founders only, Daniel Maricic and Elena Tairova"))),(0,i.kt)("li",{parentName:"ul"},"Previous grants you may have applied for.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Mentioned in ",(0,i.kt)("a",{parentName:"li",href:"#project-overview-page_facing_up"},"Project Overview"))))))}d.isMDXComponent=!0}}]);